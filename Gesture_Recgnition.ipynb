{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Gesture_Recgnition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-X53IxsnSaW"
      },
      "source": [
        "# Neural Networks Project - Gesture Recognition\n",
        "In this group project, we are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. \n",
        "\n",
        "The training data consists of a few hundred videos categorised into one of the five classes. Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images). These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use.\n",
        "\n",
        "The data is in a zip file. The zip file contains a 'train' and a 'val' folder with two CSV files for the two folders. These folders are in turn divided into subfolders where each subfolder represents a video of a particular gesture. Each subfolder, i.e. a video, contains 30 frames (or images).\n",
        "\n",
        "Our task is to train a model on the 'train' folder which performs well on the 'val' folder as well (as usually done in ML projects). We have withheld the test folder for evaluation purposes - your final model's performance will be tested on the 'test' set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEjFdLgnSag"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "import cv2\n",
        "import datetime\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LsXYP9ynSan"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxw6DQl7nSaq"
      },
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from tensorflow.keras import backend as K \n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxpFXran1n6"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87UBM8-Qn8Bl",
        "outputId": "f44f6f05-741d-424f-bec2-ec7def694ad8"
      },
      "source": [
        "#Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKOJQsVRqBdf"
      },
      "source": [
        "#Unzipping the content\n",
        "!unzip /content/drive/MyDrive/Project_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiMr8C-dnSav"
      },
      "source": [
        "#Reading the data\n",
        "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duUCmXB3Nr2"
      },
      "source": [
        "## Experiment 1 (Architecture - Conv 3D)\n",
        "(Batch Size = 10, Image dimensions = 84*84, Epochs = 20, Frames = 18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcVJIRZZzoJL"
      },
      "source": [
        "batch_size = 10\n",
        "#experimenting with batch size to use GPU to its full capacity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQkQ1NS6nSay"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdR_vXhIKGyJ"
      },
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = int(len(t)/batch_size)\n",
        "        for batch in range(num_batches):\n",
        "            batch_data = np.zeros((batch_size,18,84,84,3))\n",
        "            batch_labels = np.zeros((batch_size,5))\n",
        "            for folder in range(batch_size):\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx):\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    if image.shape[1] == 160:\n",
        "                        image = cv2.resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
        "                    else:\n",
        "                        image = cv2.resize(image,(84,84)).astype(np.float32)\n",
        "                    \n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #Red\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #Green\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #Blue\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels\n",
        "\n",
        "        if (len(t)%batch_size) != 0:\n",
        "            batch_data = np.zeros((len(t)%batch_size,18,84,84,3))\n",
        "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
        "            for folder in range(len(t)%batch_size):\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx):\n",
        "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    if image.shape[1] == 160:\n",
        "                        image = cv2.resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
        "                    else:\n",
        "                        image = cv2.resize(image,(84,84)).astype(np.float32)\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #Red\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #Green\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #Blue\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "\n",
        "            yield batch_data, batch_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB5peCdMnSa7"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK4E3MQBnSbC",
        "outputId": "47f080d4-dee0-4bf7-fea1-3a2ab85cc6b6"
      },
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = 'Project_data/train'\n",
        "val_path = 'Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 20 # choosing the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAXB14lbnSbG"
      },
      "source": [
        "## Model 1\n",
        "Here we make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6t7PuinnSbI"
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,84,84,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRx4OmU5nSbO"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oarQHH_qnSbP",
        "scrolled": true,
        "outputId": "185cd6b4-1139-4659-de6d-2d52eef02bef"
      },
      "source": [
        "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 18, 84, 84, 64)    5248      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 18, 84, 84, 64)    256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 18, 84, 84, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 9, 42, 84, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 9, 42, 84, 128)    221312    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 9, 42, 84, 128)    512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 9, 42, 84, 128)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 4, 21, 42, 128)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 4, 21, 42, 256)    884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 21, 42, 256)    1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4, 21, 42, 256)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 10, 21, 256)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 10, 21, 256)    1769728   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 10, 21, 256)    1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 10, 21, 256)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 5, 10, 256)     0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               6554112   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 9,440,773\n",
            "Trainable params: 9,439,365\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Nx0CmenSbR"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L9it_g7nSbR"
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ccebvignSbS"
      },
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
        "                             verbose=1, save_best_only=False, \n",
        "                             save_weights_only=False, mode='auto')\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor= 'val_loss', factor= 0.5, \n",
        "                       patience= 2, verbose= 10, \n",
        "                       mode= 'min', min_delta= 0.0001, \n",
        "                       cooldown= 0, min_lr= 0.0001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91AKDUegnSbT"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlVhqeetnSbT"
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UehhunAynSbU"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqRbhIRVnSbV",
        "outputId": "e9d01e47-5ce1-44f0-8fc4-2d718f5b90bc"
      },
      "source": [
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/train ; batch size = 10\n",
            "Epoch 1/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 4.5810 - categorical_accuracy: 0.3045Source path =  Project_data/val ; batch size = 10\n",
            "67/67 [==============================] - 92s 652ms/step - loss: 4.5656 - categorical_accuracy: 0.3052 - val_loss: 5.7414 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00001: saving model to model_init_2021-08-0911_03_43.393103/model-00001-3.53227-0.35143-5.74138-0.25000.h5\n",
            "Epoch 2/20\n",
            "67/67 [==============================] - 41s 619ms/step - loss: 1.7475 - categorical_accuracy: 0.4578 - val_loss: 2.2618 - val_categorical_accuracy: 0.4900\n",
            "\n",
            "Epoch 00002: saving model to model_init_2021-08-0911_03_43.393103/model-00002-1.64333-0.50830-2.26180-0.49000.h5\n",
            "Epoch 3/20\n",
            "67/67 [==============================] - 42s 626ms/step - loss: 1.4171 - categorical_accuracy: 0.5445 - val_loss: 1.3379 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: saving model to model_init_2021-08-0911_03_43.393103/model-00003-1.45283-0.52941-1.33794-0.50000.h5\n",
            "Epoch 4/20\n",
            "67/67 [==============================] - 41s 610ms/step - loss: 1.1926 - categorical_accuracy: 0.5917 - val_loss: 1.3319 - val_categorical_accuracy: 0.4800\n",
            "\n",
            "Epoch 00004: saving model to model_init_2021-08-0911_03_43.393103/model-00004-1.19555-0.58974-1.33191-0.48000.h5\n",
            "Epoch 5/20\n",
            "67/67 [==============================] - 41s 620ms/step - loss: 1.1039 - categorical_accuracy: 0.6085 - val_loss: 1.2325 - val_categorical_accuracy: 0.5800\n",
            "\n",
            "Epoch 00005: saving model to model_init_2021-08-0911_03_43.393103/model-00005-1.02983-0.62594-1.23252-0.58000.h5\n",
            "Epoch 6/20\n",
            "67/67 [==============================] - 42s 623ms/step - loss: 1.1066 - categorical_accuracy: 0.6035 - val_loss: 0.6674 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00006: saving model to model_init_2021-08-0911_03_43.393103/model-00006-1.03378-0.61840-0.66741-0.71000.h5\n",
            "Epoch 7/20\n",
            "67/67 [==============================] - 40s 606ms/step - loss: 0.8794 - categorical_accuracy: 0.6651 - val_loss: 0.7608 - val_categorical_accuracy: 0.7000\n",
            "\n",
            "Epoch 00007: saving model to model_init_2021-08-0911_03_43.393103/model-00007-0.91695-0.67873-0.76079-0.70000.h5\n",
            "Epoch 8/20\n",
            "67/67 [==============================] - 42s 631ms/step - loss: 0.8560 - categorical_accuracy: 0.6932 - val_loss: 0.7724 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00008: saving model to model_init_2021-08-0911_03_43.393103/model-00008-0.83326-0.70136-0.77241-0.69000.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 9/20\n",
            "67/67 [==============================] - 41s 617ms/step - loss: 0.7135 - categorical_accuracy: 0.7379 - val_loss: 0.5489 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00009: saving model to model_init_2021-08-0911_03_43.393103/model-00009-0.68375-0.75415-0.54893-0.80000.h5\n",
            "Epoch 10/20\n",
            "67/67 [==============================] - 42s 627ms/step - loss: 0.5890 - categorical_accuracy: 0.7407 - val_loss: 0.6038 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00010: saving model to model_init_2021-08-0911_03_43.393103/model-00010-0.55289-0.77526-0.60378-0.77000.h5\n",
            "Epoch 11/20\n",
            "67/67 [==============================] - 41s 612ms/step - loss: 0.5295 - categorical_accuracy: 0.7904 - val_loss: 0.5476 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00011: saving model to model_init_2021-08-0911_03_43.393103/model-00011-0.54703-0.77074-0.54760-0.75000.h5\n",
            "Epoch 12/20\n",
            "67/67 [==============================] - 42s 625ms/step - loss: 0.4607 - categorical_accuracy: 0.8271 - val_loss: 0.6462 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00012: saving model to model_init_2021-08-0911_03_43.393103/model-00012-0.53118-0.79638-0.64617-0.75000.h5\n",
            "Epoch 13/20\n",
            "67/67 [==============================] - 41s 614ms/step - loss: 0.4789 - categorical_accuracy: 0.8232 - val_loss: 0.4700 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00013: saving model to model_init_2021-08-0911_03_43.393103/model-00013-0.51329-0.80995-0.47005-0.78000.h5\n",
            "Epoch 14/20\n",
            "67/67 [==============================] - 41s 608ms/step - loss: 0.4661 - categorical_accuracy: 0.8193 - val_loss: 0.4410 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00014: saving model to model_init_2021-08-0911_03_43.393103/model-00014-0.47216-0.81750-0.44104-0.82000.h5\n",
            "Epoch 15/20\n",
            "67/67 [==============================] - 42s 625ms/step - loss: 0.3933 - categorical_accuracy: 0.8508 - val_loss: 0.8974 - val_categorical_accuracy: 0.6700\n",
            "\n",
            "Epoch 00015: saving model to model_init_2021-08-0911_03_43.393103/model-00015-0.45904-0.82655-0.89736-0.67000.h5\n",
            "Epoch 16/20\n",
            "67/67 [==============================] - 42s 626ms/step - loss: 0.3950 - categorical_accuracy: 0.8533 - val_loss: 0.4279 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00016: saving model to model_init_2021-08-0911_03_43.393103/model-00016-0.40771-0.84766-0.42788-0.81000.h5\n",
            "Epoch 17/20\n",
            "67/67 [==============================] - 41s 616ms/step - loss: 0.3603 - categorical_accuracy: 0.8582 - val_loss: 0.5109 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00017: saving model to model_init_2021-08-0911_03_43.393103/model-00017-0.39164-0.84465-0.51088-0.79000.h5\n",
            "Epoch 18/20\n",
            "67/67 [==============================] - 42s 630ms/step - loss: 0.4142 - categorical_accuracy: 0.8548 - val_loss: 0.7352 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00018: saving model to model_init_2021-08-0911_03_43.393103/model-00018-0.41576-0.84465-0.73518-0.80000.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 19/20\n",
            "67/67 [==============================] - 42s 635ms/step - loss: 0.3672 - categorical_accuracy: 0.8393 - val_loss: 0.5228 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00019: saving model to model_init_2021-08-0911_03_43.393103/model-00019-0.35480-0.85520-0.52279-0.78000.h5\n",
            "Epoch 20/20\n",
            "67/67 [==============================] - 41s 612ms/step - loss: 0.3036 - categorical_accuracy: 0.9102 - val_loss: 0.4809 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00020: saving model to model_init_2021-08-0911_03_43.393103/model-00020-0.35592-0.88084-0.48095-0.81000.h5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8c055eb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2buD5Y64CKA"
      },
      "source": [
        "#### Maximum validation accuracy = 82%\n",
        "#### Corresponding training accuracy = 81.93%\n",
        "\n",
        "The model already has a good enough accuracy. We are going to experiment with the batch size by increasing it to 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CImwYfPF4nNJ"
      },
      "source": [
        "## Experiment 2 (Architecture - Conv 3D)\n",
        "\n",
        "(Batch Size = 10, Image dimensions = 84*84, Epochs = 20, Frames = 18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TT5j61qh_Ii"
      },
      "source": [
        "batch_size = 32\n",
        "#experimenting with batch size to use GPU to its full capacity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Feqd-N62h_Ik"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7nECfEgh_In"
      },
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = int(len(t)/batch_size)\n",
        "        for batch in range(num_batches):\n",
        "            batch_data = np.zeros((batch_size,18,84,84,3))\n",
        "            batch_labels = np.zeros((batch_size,5))\n",
        "            for folder in range(batch_size):\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx):\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    if image.shape[1] == 160:\n",
        "                        image = cv2.resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
        "                    else:\n",
        "                        image = cv2.resize(image,(84,84)).astype(np.float32)\n",
        "                    \n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #Red\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #Green\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #Blue\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels\n",
        "\n",
        "        if (len(t)%batch_size) != 0:\n",
        "            batch_data = np.zeros((len(t)%batch_size,18,84,84,3))\n",
        "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
        "            for folder in range(len(t)%batch_size):\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx):\n",
        "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    if image.shape[1] == 160:\n",
        "                        image = cv2.resize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
        "                    else:\n",
        "                        image = cv2.resize(image,(84,84)).astype(np.float32)\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #Red\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #Green\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #Blue\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "\n",
        "            yield batch_data, batch_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZtSyOz3h_Ip"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aa1SChJh_Iq",
        "outputId": "0977de20-3884-48a7-d845-a80a615d8ab8"
      },
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = 'Project_data/train'\n",
        "val_path = 'Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 20 # choosing the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpUUuAafh_Is"
      },
      "source": [
        "## Model 2\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtnY-JI-h_Iu"
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,84,84,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxT6wDSqh_Iu"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "scrolled": true,
        "id": "6bVJZV08h_Iv",
        "outputId": "a60ccd25-cac7-4cc8-c203-656e99139cfa"
      },
      "source": [
        "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_4 (Conv3D)            (None, 18, 84, 84, 64)    5248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 18, 84, 84, 64)    256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 18, 84, 84, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3 (None, 9, 42, 84, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 9, 42, 84, 128)    221312    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 42, 84, 128)    512       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 42, 84, 128)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_5 (MaxPooling3 (None, 4, 21, 42, 128)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 4, 21, 42, 256)    884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 21, 42, 256)    1024      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4, 21, 42, 256)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3 (None, 2, 10, 21, 256)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 2, 10, 21, 256)    1769728   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 2, 10, 21, 256)    1024      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 2, 10, 21, 256)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3 (None, 1, 5, 10, 256)     0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               6554112   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 9,440,773\n",
            "Trainable params: 9,439,365\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfwb_P3ih_Iw"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qLJHY3rh_Ix"
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBZwaYZ-h_Iy"
      },
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
        "                             verbose=1, save_best_only=False, \n",
        "                             save_weights_only=False, mode='auto')\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor= 'val_loss', factor= 0.5, \n",
        "                       patience= 2, verbose= 10, \n",
        "                       mode= 'min', min_delta= 0.0001, \n",
        "                       cooldown= 0, min_lr= 0.0001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKU2U-lFh_Iy"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QytrMrjkh_Iz"
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EhQ2Qa6h_Iz"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaAJe982h_I0",
        "outputId": "006e8d9f-0217-4a76-d268-67a532608ba9"
      },
      "source": [
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/train ; batch size = 32\n",
            "Epoch 1/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 4.2054 - categorical_accuracy: 0.2597Source path =  Project_data/val ; batch size = 32\n",
            "21/21 [==============================] - 52s 2s/step - loss: 4.1651 - categorical_accuracy: 0.2621 - val_loss: 11.0375 - val_categorical_accuracy: 0.2100\n",
            "\n",
            "Epoch 00001: saving model to model_init_2021-08-0911_19_29.133050/model-00001-3.31934-0.31071-11.03752-0.21000.h5\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.9188 - categorical_accuracy: 0.4638 - val_loss: 7.6775 - val_categorical_accuracy: 0.2400\n",
            "\n",
            "Epoch 00002: saving model to model_init_2021-08-0911_19_29.133050/model-00002-1.79857-0.48869-7.67753-0.24000.h5\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.4691 - categorical_accuracy: 0.5615 - val_loss: 6.9134 - val_categorical_accuracy: 0.2100\n",
            "\n",
            "Epoch 00003: saving model to model_init_2021-08-0911_19_29.133050/model-00003-1.35762-0.57919-6.91338-0.21000.h5\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 39s 2s/step - loss: 1.2548 - categorical_accuracy: 0.5684 - val_loss: 3.5823 - val_categorical_accuracy: 0.2700\n",
            "\n",
            "Epoch 00004: saving model to model_init_2021-08-0911_19_29.133050/model-00004-1.22563-0.57768-3.58229-0.27000.h5\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.1167 - categorical_accuracy: 0.5981 - val_loss: 2.2696 - val_categorical_accuracy: 0.4200\n",
            "\n",
            "Epoch 00005: saving model to model_init_2021-08-0911_19_29.133050/model-00005-1.08157-0.62293-2.26958-0.42000.h5\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.0539 - categorical_accuracy: 0.6370 - val_loss: 1.7896 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00006: saving model to model_init_2021-08-0911_19_29.133050/model-00006-1.02379-0.63198-1.78955-0.50000.h5\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.9271 - categorical_accuracy: 0.6972 - val_loss: 1.0235 - val_categorical_accuracy: 0.6500\n",
            "\n",
            "Epoch 00007: saving model to model_init_2021-08-0911_19_29.133050/model-00007-0.95856-0.68477-1.02349-0.65000.h5\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7873 - categorical_accuracy: 0.7208 - val_loss: 0.9297 - val_categorical_accuracy: 0.6100\n",
            "\n",
            "Epoch 00008: saving model to model_init_2021-08-0911_19_29.133050/model-00008-0.80430-0.71644-0.92966-0.61000.h5\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.7844 - categorical_accuracy: 0.7036 - val_loss: 0.8368 - val_categorical_accuracy: 0.6700\n",
            "\n",
            "Epoch 00009: saving model to model_init_2021-08-0911_19_29.133050/model-00009-0.71606-0.73906-0.83683-0.67000.h5\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.7496 - categorical_accuracy: 0.7117 - val_loss: 0.8443 - val_categorical_accuracy: 0.6600\n",
            "\n",
            "Epoch 00010: saving model to model_init_2021-08-0911_19_29.133050/model-00010-0.72306-0.71795-0.84429-0.66000.h5\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7061 - categorical_accuracy: 0.7456 - val_loss: 0.7401 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00011: saving model to model_init_2021-08-0911_19_29.133050/model-00011-0.66473-0.75867-0.74011-0.74000.h5\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.6493 - categorical_accuracy: 0.7420 - val_loss: 0.8688 - val_categorical_accuracy: 0.7200\n",
            "\n",
            "Epoch 00012: saving model to model_init_2021-08-0911_19_29.133050/model-00012-0.60318-0.75566-0.86884-0.72000.h5\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.5573 - categorical_accuracy: 0.8030 - val_loss: 0.8314 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00013: saving model to model_init_2021-08-0911_19_29.133050/model-00013-0.56818-0.78431-0.83142-0.69000.h5\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.4992 - categorical_accuracy: 0.8095 - val_loss: 0.6978 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00014: saving model to model_init_2021-08-0911_19_29.133050/model-00014-0.52315-0.80090-0.69785-0.76000.h5\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.4746 - categorical_accuracy: 0.8039 - val_loss: 0.5885 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00015: saving model to model_init_2021-08-0911_19_29.133050/model-00015-0.48194-0.81448-0.58849-0.76000.h5\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 39s 2s/step - loss: 0.5592 - categorical_accuracy: 0.8128 - val_loss: 0.7008 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00016: saving model to model_init_2021-08-0911_19_29.133050/model-00016-0.52068-0.81297-0.70083-0.73000.h5\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.4998 - categorical_accuracy: 0.8074 - val_loss: 0.6758 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00017: saving model to model_init_2021-08-0911_19_29.133050/model-00017-0.47775-0.81599-0.67584-0.76000.h5\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.4819 - categorical_accuracy: 0.8173 - val_loss: 0.7537 - val_categorical_accuracy: 0.6900\n",
            "\n",
            "Epoch 00018: saving model to model_init_2021-08-0911_19_29.133050/model-00018-0.41327-0.84615-0.75371-0.69000.h5\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.3260 - categorical_accuracy: 0.8763 - val_loss: 0.5996 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00019: saving model to model_init_2021-08-0911_19_29.133050/model-00019-0.34196-0.87179-0.59960-0.77000.h5\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 39s 2s/step - loss: 0.3360 - categorical_accuracy: 0.8739 - val_loss: 0.6457 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00020: saving model to model_init_2021-08-0911_19_29.133050/model-00020-0.36196-0.85973-0.64571-0.77000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8d4fa7d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bARvdS_65CGu"
      },
      "source": [
        "### Maximum validation accuracy : 77%\n",
        "### Corresponding testing accuracy : 87.39%\n",
        "\n",
        "The accuracy score has decreased but the process was faster after increasing the batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2MucDw_5fOc"
      },
      "source": [
        "## Experiment 3\n",
        "(Architecture - Conv 3D)\n",
        "\n",
        "(Batch Size = 10, Image dimensions = 120*120, Epochs = 50, Frames = 20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F03Za-gsctJ"
      },
      "source": [
        "batch_size = 10\n",
        "#experimenting with batch size to use GPU to its full capacity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-t0J8zPsctQ"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLqVq9P0sctS"
      },
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = int(len(t)/batch_size)\n",
        "        for batch in range(num_batches):\n",
        "            batch_data = np.zeros((batch_size,18,120,120,3))\n",
        "            batch_labels = np.zeros((batch_size,5))\n",
        "            for folder in range(batch_size):\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx):\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    if image.shape[1] == 160:\n",
        "                        image = cv2.resize(image[:,20:140,:],(120,120)).astype(np.float32)\n",
        "                    else:\n",
        "                        image = cv2.resize(image,(120,120)).astype(np.float32)\n",
        "                    \n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #Red\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #Green\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #Blue\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels\n",
        "\n",
        "        if (len(t)%batch_size) != 0:\n",
        "            batch_data = np.zeros((len(t)%batch_size,18,120,120,3))\n",
        "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
        "            for folder in range(len(t)%batch_size):\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx):\n",
        "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    if image.shape[1] == 160:\n",
        "                        image = cv2.resize(image[:,20:140,:],(120,120)).astype(np.float32)\n",
        "                    else:\n",
        "                        image = cv2.resize(image,(120,120)).astype(np.float32)\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #Red\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0 #Green\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0 #Blue\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "\n",
        "            yield batch_data, batch_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aIHfQjUsctV"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0DbwSN1sctW",
        "outputId": "8f3eece1-612a-4960-fd26-97e0f3b5b23a"
      },
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = 'Project_data/train'\n",
        "val_path = 'Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 50 # choosing the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGp1vYBqsctX"
      },
      "source": [
        "## Model 3\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-XCEJvqsctZ"
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,120,120,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKbGblqIscta"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "scrolled": true,
        "id": "GTP1Znoasctb",
        "outputId": "f75b5651-5476-42b3-b9a5-101f9d50452c"
      },
      "source": [
        "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_8 (Conv3D)            (None, 18, 120, 120, 64)  5248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 18, 120, 120, 64)  256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 18, 120, 120, 64)  0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_8 (MaxPooling3 (None, 9, 60, 120, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_9 (Conv3D)            (None, 9, 60, 120, 128)   221312    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 9, 60, 120, 128)   512       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 9, 60, 120, 128)   0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3 (None, 4, 30, 60, 128)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_10 (Conv3D)           (None, 4, 30, 60, 256)    884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 30, 60, 256)    1024      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 4, 30, 60, 256)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_10 (MaxPooling (None, 2, 15, 30, 256)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_11 (Conv3D)           (None, 2, 15, 30, 256)    1769728   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 2, 15, 30, 256)    1024      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 2, 15, 30, 256)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_11 (MaxPooling (None, 1, 7, 15, 256)     0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 26880)             0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 26880)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               13763072  \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 16,649,733\n",
            "Trainable params: 16,648,325\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BlVO_4Vsctc"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46kwNQWnscte"
      },
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6r-i0fVsctg"
      },
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
        "                             verbose=1, save_best_only=False, \n",
        "                             save_weights_only=False, mode='auto')\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor= 'val_loss', factor= 0.5, \n",
        "                       patience= 2, verbose= 10, \n",
        "                       mode= 'min', min_delta= 0.0001, \n",
        "                       cooldown= 0, min_lr= 0.0001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8I5C6x0sctg"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM0Wc7IOscti"
      },
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6yU-rXscti"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPG8QNcesctj",
        "outputId": "0e44ebe1-2e1f-4f5b-f66a-c5e5281e7503"
      },
      "source": [
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/train ; batch size = 10\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - ETA: 0s - loss: 5.1298 - categorical_accuracy: 0.3139Source path =  Project_data/val ; batch size = 10\n",
            "67/67 [==============================] - 70s 957ms/step - loss: 5.1038 - categorical_accuracy: 0.3149 - val_loss: 4.6036 - val_categorical_accuracy: 0.2700\n",
            "\n",
            "Epoch 00001: saving model to model_init_2021-08-0911_40_30.295443/model-00001-3.36651-0.38009-4.60359-0.27000.h5\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 62s 922ms/step - loss: 1.5944 - categorical_accuracy: 0.5519 - val_loss: 1.2889 - val_categorical_accuracy: 0.5500\n",
            "\n",
            "Epoch 00002: saving model to model_init_2021-08-0911_40_30.295443/model-00002-1.43341-0.57617-1.28888-0.55000.h5\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 62s 919ms/step - loss: 1.0550 - categorical_accuracy: 0.6224 - val_loss: 1.8560 - val_categorical_accuracy: 0.4600\n",
            "\n",
            "Epoch 00003: saving model to model_init_2021-08-0911_40_30.295443/model-00003-1.09862-0.61237-1.85598-0.46000.h5\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 61s 919ms/step - loss: 0.9923 - categorical_accuracy: 0.6662 - val_loss: 1.1682 - val_categorical_accuracy: 0.5800\n",
            "\n",
            "Epoch 00004: saving model to model_init_2021-08-0911_40_30.295443/model-00004-1.03714-0.64103-1.16823-0.58000.h5\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.8627 - categorical_accuracy: 0.6958 - val_loss: 0.7172 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00005: saving model to model_init_2021-08-0911_40_30.295443/model-00005-0.95584-0.66667-0.71718-0.73000.h5\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 62s 922ms/step - loss: 0.7475 - categorical_accuracy: 0.7229 - val_loss: 0.7636 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00006: saving model to model_init_2021-08-0911_40_30.295443/model-00006-0.77860-0.70136-0.76363-0.77000.h5\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 62s 922ms/step - loss: 0.7186 - categorical_accuracy: 0.7474 - val_loss: 1.4874 - val_categorical_accuracy: 0.6300\n",
            "\n",
            "Epoch 00007: saving model to model_init_2021-08-0911_40_30.295443/model-00007-0.78424-0.73002-1.48736-0.63000.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 61s 919ms/step - loss: 0.9609 - categorical_accuracy: 0.7051 - val_loss: 0.5038 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00008: saving model to model_init_2021-08-0911_40_30.295443/model-00008-0.69933-0.75113-0.50382-0.83000.h5\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 61s 913ms/step - loss: 0.4610 - categorical_accuracy: 0.8442 - val_loss: 0.6393 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00009: saving model to model_init_2021-08-0911_40_30.295443/model-00009-0.50674-0.81599-0.63928-0.77000.h5\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 61s 918ms/step - loss: 0.5125 - categorical_accuracy: 0.8049 - val_loss: 0.6531 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00010: saving model to model_init_2021-08-0911_40_30.295443/model-00010-0.51393-0.80694-0.65313-0.76000.h5\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.4361 - categorical_accuracy: 0.8281 - val_loss: 0.6026 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00011: saving model to model_init_2021-08-0911_40_30.295443/model-00011-0.43463-0.82655-0.60260-0.78000.h5\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 62s 922ms/step - loss: 0.4581 - categorical_accuracy: 0.8142 - val_loss: 0.6110 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00012: saving model to model_init_2021-08-0911_40_30.295443/model-00012-0.39150-0.85068-0.61096-0.78000.h5\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 62s 924ms/step - loss: 0.3257 - categorical_accuracy: 0.8887 - val_loss: 0.4908 - val_categorical_accuracy: 0.8300\n",
            "\n",
            "Epoch 00013: saving model to model_init_2021-08-0911_40_30.295443/model-00013-0.30585-0.88235-0.49076-0.83000.h5\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 62s 922ms/step - loss: 0.3208 - categorical_accuracy: 0.8782 - val_loss: 0.6130 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00014: saving model to model_init_2021-08-0911_40_30.295443/model-00014-0.30504-0.88235-0.61303-0.73000.h5\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 62s 921ms/step - loss: 0.3714 - categorical_accuracy: 0.8633 - val_loss: 0.5062 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00015: saving model to model_init_2021-08-0911_40_30.295443/model-00015-0.35367-0.87481-0.50623-0.78000.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.3560 - categorical_accuracy: 0.8617 - val_loss: 0.6242 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00016: saving model to model_init_2021-08-0911_40_30.295443/model-00016-0.30186-0.88989-0.62419-0.73000.h5\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 61s 916ms/step - loss: 0.3103 - categorical_accuracy: 0.8902 - val_loss: 0.5615 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00017: saving model to model_init_2021-08-0911_40_30.295443/model-00017-0.27023-0.90498-0.56151-0.79000.h5\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 62s 923ms/step - loss: 0.3318 - categorical_accuracy: 0.8917 - val_loss: 0.6639 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00018: saving model to model_init_2021-08-0911_40_30.295443/model-00018-0.33534-0.88839-0.66391-0.74000.h5\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 62s 923ms/step - loss: 0.2706 - categorical_accuracy: 0.8907 - val_loss: 0.5742 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00019: saving model to model_init_2021-08-0911_40_30.295443/model-00019-0.30097-0.87783-0.57418-0.76000.h5\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 62s 921ms/step - loss: 0.3035 - categorical_accuracy: 0.8774 - val_loss: 0.5200 - val_categorical_accuracy: 0.8100\n",
            "\n",
            "Epoch 00020: saving model to model_init_2021-08-0911_40_30.295443/model-00020-0.31565-0.87481-0.51996-0.81000.h5\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 62s 920ms/step - loss: 0.2934 - categorical_accuracy: 0.8961 - val_loss: 0.5720 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00021: saving model to model_init_2021-08-0911_40_30.295443/model-00021-0.28177-0.89442-0.57201-0.77000.h5\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 62s 921ms/step - loss: 0.3234 - categorical_accuracy: 0.8799 - val_loss: 0.5904 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00022: saving model to model_init_2021-08-0911_40_30.295443/model-00022-0.29143-0.89140-0.59044-0.74000.h5\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 61s 911ms/step - loss: 0.2467 - categorical_accuracy: 0.9191 - val_loss: 0.5198 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00023: saving model to model_init_2021-08-0911_40_30.295443/model-00023-0.26347-0.90498-0.51976-0.78000.h5\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 62s 923ms/step - loss: 0.2785 - categorical_accuracy: 0.8919 - val_loss: 0.5034 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00024: saving model to model_init_2021-08-0911_40_30.295443/model-00024-0.28048-0.88537-0.50344-0.78000.h5\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 61s 916ms/step - loss: 0.2398 - categorical_accuracy: 0.9153 - val_loss: 0.6330 - val_categorical_accuracy: 0.7100\n",
            "\n",
            "Epoch 00025: saving model to model_init_2021-08-0911_40_30.295443/model-00025-0.26527-0.90347-0.63297-0.71000.h5\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 62s 921ms/step - loss: 0.2748 - categorical_accuracy: 0.9165 - val_loss: 0.6291 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00026: saving model to model_init_2021-08-0911_40_30.295443/model-00026-0.26533-0.90649-0.62908-0.75000.h5\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 62s 925ms/step - loss: 0.2500 - categorical_accuracy: 0.9078 - val_loss: 0.4893 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00027: saving model to model_init_2021-08-0911_40_30.295443/model-00027-0.21668-0.92157-0.48928-0.78000.h5\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 62s 922ms/step - loss: 0.2850 - categorical_accuracy: 0.8884 - val_loss: 0.5830 - val_categorical_accuracy: 0.8000\n",
            "\n",
            "Epoch 00028: saving model to model_init_2021-08-0911_40_30.295443/model-00028-0.28900-0.89291-0.58298-0.80000.h5\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.2794 - categorical_accuracy: 0.8856 - val_loss: 0.6217 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00029: saving model to model_init_2021-08-0911_40_30.295443/model-00029-0.26286-0.89291-0.62169-0.74000.h5\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.2911 - categorical_accuracy: 0.8931 - val_loss: 0.5378 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00030: saving model to model_init_2021-08-0911_40_30.295443/model-00030-0.29905-0.88235-0.53781-0.76000.h5\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 61s 916ms/step - loss: 0.2389 - categorical_accuracy: 0.9121 - val_loss: 0.5775 - val_categorical_accuracy: 0.7500\n",
            "\n",
            "Epoch 00031: saving model to model_init_2021-08-0911_40_30.295443/model-00031-0.25111-0.90347-0.57749-0.75000.h5\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.2198 - categorical_accuracy: 0.8989 - val_loss: 0.5655 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00032: saving model to model_init_2021-08-0911_40_30.295443/model-00032-0.21785-0.90347-0.56548-0.77000.h5\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 62s 924ms/step - loss: 0.2155 - categorical_accuracy: 0.9185 - val_loss: 0.5082 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00033: saving model to model_init_2021-08-0911_40_30.295443/model-00033-0.21675-0.91403-0.50823-0.79000.h5\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 61s 913ms/step - loss: 0.2817 - categorical_accuracy: 0.8904 - val_loss: 0.6608 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00034: saving model to model_init_2021-08-0911_40_30.295443/model-00034-0.24360-0.91101-0.66083-0.73000.h5\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 61s 916ms/step - loss: 0.2210 - categorical_accuracy: 0.9119 - val_loss: 0.5508 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00035: saving model to model_init_2021-08-0911_40_30.295443/model-00035-0.22706-0.91855-0.55078-0.78000.h5\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 61s 913ms/step - loss: 0.1855 - categorical_accuracy: 0.9414 - val_loss: 0.5993 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00036: saving model to model_init_2021-08-0911_40_30.295443/model-00036-0.20263-0.92006-0.59934-0.74000.h5\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 61s 917ms/step - loss: 0.2486 - categorical_accuracy: 0.9050 - val_loss: 0.7049 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00037: saving model to model_init_2021-08-0911_40_30.295443/model-00037-0.20389-0.93062-0.70487-0.73000.h5\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 62s 920ms/step - loss: 0.2422 - categorical_accuracy: 0.9062 - val_loss: 0.5614 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00038: saving model to model_init_2021-08-0911_40_30.295443/model-00038-0.21912-0.91403-0.56136-0.77000.h5\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 61s 918ms/step - loss: 0.1936 - categorical_accuracy: 0.9265 - val_loss: 0.5514 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00039: saving model to model_init_2021-08-0911_40_30.295443/model-00039-0.20201-0.92459-0.55136-0.76000.h5\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 61s 915ms/step - loss: 0.1764 - categorical_accuracy: 0.9291 - val_loss: 0.5571 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00040: saving model to model_init_2021-08-0911_40_30.295443/model-00040-0.17361-0.93514-0.55707-0.82000.h5\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 61s 914ms/step - loss: 0.2114 - categorical_accuracy: 0.9216 - val_loss: 0.5768 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00041: saving model to model_init_2021-08-0911_40_30.295443/model-00041-0.21294-0.91704-0.57684-0.77000.h5\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 62s 921ms/step - loss: 0.2639 - categorical_accuracy: 0.9012 - val_loss: 0.6894 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00042: saving model to model_init_2021-08-0911_40_30.295443/model-00042-0.22997-0.91704-0.68942-0.73000.h5\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 61s 912ms/step - loss: 0.2272 - categorical_accuracy: 0.9143 - val_loss: 0.5362 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00043: saving model to model_init_2021-08-0911_40_30.295443/model-00043-0.23180-0.91403-0.53616-0.79000.h5\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 62s 923ms/step - loss: 0.2409 - categorical_accuracy: 0.9233 - val_loss: 0.6503 - val_categorical_accuracy: 0.7600\n",
            "\n",
            "Epoch 00044: saving model to model_init_2021-08-0911_40_30.295443/model-00044-0.19705-0.93665-0.65032-0.76000.h5\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 62s 926ms/step - loss: 0.1699 - categorical_accuracy: 0.9387 - val_loss: 0.5573 - val_categorical_accuracy: 0.7900\n",
            "\n",
            "Epoch 00045: saving model to model_init_2021-08-0911_40_30.295443/model-00045-0.18053-0.92459-0.55734-0.79000.h5\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 62s 920ms/step - loss: 0.1502 - categorical_accuracy: 0.9486 - val_loss: 0.6042 - val_categorical_accuracy: 0.7400\n",
            "\n",
            "Epoch 00046: saving model to model_init_2021-08-0911_40_30.295443/model-00046-0.18673-0.92760-0.60422-0.74000.h5\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 61s 911ms/step - loss: 0.1889 - categorical_accuracy: 0.9285 - val_loss: 0.5971 - val_categorical_accuracy: 0.7800\n",
            "\n",
            "Epoch 00047: saving model to model_init_2021-08-0911_40_30.295443/model-00047-0.18929-0.92760-0.59706-0.78000.h5\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 61s 912ms/step - loss: 0.1583 - categorical_accuracy: 0.9448 - val_loss: 0.4442 - val_categorical_accuracy: 0.8200\n",
            "\n",
            "Epoch 00048: saving model to model_init_2021-08-0911_40_30.295443/model-00048-0.16033-0.94419-0.44415-0.82000.h5\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 61s 919ms/step - loss: 0.1752 - categorical_accuracy: 0.9290 - val_loss: 0.7234 - val_categorical_accuracy: 0.7300\n",
            "\n",
            "Epoch 00049: saving model to model_init_2021-08-0911_40_30.295443/model-00049-0.15579-0.93967-0.72336-0.73000.h5\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 61s 918ms/step - loss: 0.1553 - categorical_accuracy: 0.9473 - val_loss: 0.5615 - val_categorical_accuracy: 0.7700\n",
            "\n",
            "Epoch 00050: saving model to model_init_2021-08-0911_40_30.295443/model-00050-0.16133-0.94118-0.56152-0.77000.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff878128710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnp8JzX96WaJ"
      },
      "source": [
        "### Maximum validation accuracy : 83%\n",
        "### Corresponding testing accuracy : 88% (Epoch 13)\n",
        "\n",
        "The accuracy score has increased for both Maximum validation accuaracy and testing accuracy.\n",
        "Also, increase in epoch did not necessarily reuslt in increase of the score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNFsrbiPFSrk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}